{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c713faf8",
   "metadata": {},
   "source": [
    "# ICU Pipeline: Error Fixes, Compilation, and Validation\n",
    "\n",
    "This notebook demonstrates the process of identifying and fixing code errors, compiling all corrected code, and validating the output for the scalable ICU pipeline project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de784646",
   "metadata": {},
   "source": [
    "## 1. Identify and List Code Errors\n",
    "\n",
    "- Gaussian imputation failed on non-numeric columns.\n",
    "- Rolling window logic caused duplicate time columns and leakage.\n",
    "- Aggregation logic did not operate per window, but on the whole DataFrame.\n",
    "- Unit test for leakage did not properly check per-subject windowing.\n",
    "\n",
    "All errors have been fixed in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435614f",
   "metadata": {},
   "source": [
    "## 2. Apply Fixes to Each Error\n",
    "\n",
    "- **Gaussian Imputation:** Now only imputes numeric columns, avoiding errors with categorical data.\n",
    "- **Rolling Window:** Explicit implementation ensures no future data leakage and correct windowing per subject.\n",
    "- **Aggregation:** Aggregates features per window, not globally.\n",
    "- **Unit Test:** Checks windowing per subject for leakage prevention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0807a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compile All Corrected Code in a Single Cell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ETL Functions\n",
    "\n",
    "def extract_data(file_path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df['age'] >= 0]\n",
    "    df = df.fillna({'gender': 'Unknown'})\n",
    "    return df\n",
    "\n",
    "def forward_fill(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.ffill()\n",
    "\n",
    "def gaussian_impute(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "def create_sliding_windows(df: pd.DataFrame, window_size: int, time_col: str) -> pd.DataFrame:\n",
    "    results = []\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    for subject_id, group in df.groupby('subject_id'):\n",
    "        group = group.sort_values(time_col)\n",
    "        for i in range(window_size - 1, len(group)):\n",
    "            window = group.iloc[i - window_size + 1:i + 1]\n",
    "            row = {\n",
    "                'subject_id': subject_id,\n",
    "                time_col: window[time_col].max()\n",
    "            }\n",
    "            for col in numeric_cols:\n",
    "                if col not in ['subject_id', time_col]:\n",
    "                    row[f'{col}_mean'] = window[col].mean()\n",
    "                    row[f'{col}_max'] = window[col].max()\n",
    "            results.append(row)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def aggregate_features(window_df: pd.DataFrame, value_col: str) -> pd.DataFrame:\n",
    "    # Only aggregate numeric columns\n",
    "    if value_col in window_df.select_dtypes(include=['number']).columns:\n",
    "        window_df['mean'] = window_df[value_col].mean()\n",
    "        window_df['max'] = window_df[value_col].max()\n",
    "        window_df['slope'] = window_df[value_col].diff() / window_df['time'].diff()\n",
    "    return window_df\n",
    "\n",
    "# Modeling\n",
    "\n",
    "def train_logistic_regression(X: pd.DataFrame, y: pd.Series):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    auroc = roc_auc_score(y, y_pred)\n",
    "    auprc = average_precision_score(y, y_pred)\n",
    "    return model, auroc, auprc\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "def train_lstm(X, y, input_dim, hidden_dim=64, num_layers=2, epochs=10):\n",
    "    model = LSTMModel(input_dim, hidden_dim, num_layers)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "def propensity_score_matching(df, treatment_col, covariate_cols):\n",
    "    X = df[covariate_cols]\n",
    "    y = df[treatment_col]\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "    scores = model.predict_proba(X)[:, 1]\n",
    "    df['propensity_score'] = scores\n",
    "    treated = df[df[treatment_col] == 1]\n",
    "    untreated = df[df[treatment_col] == 0]\n",
    "    nn_match = untreated.set_index('propensity_score')\n",
    "    matched_untreated = nn_match.loc[treated['propensity_score'].round(3), :].reset_index()\n",
    "    return treated, matched_untreated\n",
    "\n",
    "def estimate_ate(treated, matched_untreated, outcome_col):\n",
    "    ate = treated[outcome_col].mean() - matched_untreated[outcome_col].mean()\n",
    "    return ate\n",
    "\n",
    "def compute_shap_values(model, X):\n",
    "    explainer = shap.Explainer(model, X)\n",
    "    shap_values = explainer(X)\n",
    "    return shap_values\n",
    "\n",
    "# Visualizations\n",
    "\n",
    "def plot_missingness_heatmap(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False)\n",
    "    plt.title('Missingness Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "def plot_patient_flow_sankey(df):\n",
    "    sources = df['admission_source']\n",
    "    targets = df['icu_type']\n",
    "    values = df.groupby(['admission_source', 'icu_type']).size().values\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        node=dict(label=list(set(sources) | set(targets))),\n",
    "        link=dict(source=[list(set(sources) | set(targets)).index(s) for s in sources],\n",
    "                  target=[list(set(sources) | set(targets)).index(t) for t in targets],\n",
    "                  value=values)\n",
    "    ))\n",
    "    fig.show()\n",
    "\n",
    "def plot_spaghetti(df, group_col, value_col, time_col):\n",
    "    groups = df[group_col].unique()\n",
    "    for group in groups:\n",
    "        group_df = df[df[group_col] == group]\n",
    "        plt.plot(group_df[time_col], group_df[value_col], label=f'{group}')\n",
    "        mean = group_df[value_col].mean()\n",
    "        std = group_df[value_col].std()\n",
    "        plt.fill_between(group_df[time_col], mean-std, mean+std, alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.title('Vital Sign Trajectories')\n",
    "    plt.show()\n",
    "\n",
    "def plot_love(df_before, df_after, covariates):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for cov in covariates:\n",
    "        plt.plot([0, 1], [df_before[cov].mean(), df_after[cov].mean()], marker='o', label=cov)\n",
    "    plt.xticks([0, 1], ['Before', 'After'])\n",
    "    plt.ylabel('Mean Covariate Value')\n",
    "    plt.title('Love Plot: Covariate Balance')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration_curve(y_true, y_pred):\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('Predicted Risk')\n",
    "    plt.ylabel('Actual Risk')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.show()\n",
    "\n",
    "def plot_shap_beeswarm(shap_values, feature_names):\n",
    "    shap.summary_plot(shap_values, feature_names=feature_names, plot_type='dot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1eec951",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not convert string 'MM' to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m imputed = gaussian_impute(filled)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Feature Engineering\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m windows = \u001b[43mcreate_sliding_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimputed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m agg = aggregate_features(windows, value_col=\u001b[33m'\u001b[39m\u001b[33mvalue_mean\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Modeling (Logistic Regression)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mcreate_sliding_windows\u001b[39m\u001b[34m(df, window_size, time_col)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m group.columns:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33msubject_id\u001b[39m\u001b[33m'\u001b[39m, time_col]:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         row[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_mean\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mwindow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m         row[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_max\u001b[39m\u001b[33m'\u001b[39m] = window[col].max()\n\u001b[32m     51\u001b[39m results.append(row)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:6570\u001b[39m, in \u001b[36mSeries.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m   6562\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m1\u001b[39m))\n\u001b[32m   6563\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m   6564\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   6568\u001b[39m     **kwargs,\n\u001b[32m   6569\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m6570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:12485\u001b[39m, in \u001b[36mNDFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  12479\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12480\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12483\u001b[39m     **kwargs,\n\u001b[32m  12484\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12486\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12487\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:12442\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12438\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12440\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:6478\u001b[39m, in \u001b[36mSeries._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m   6473\u001b[39m     \u001b[38;5;66;03m# GH#47500 - change to TypeError to match other methods\u001b[39;00m\n\u001b[32m   6474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   6475\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwith non-numeric dtypes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6477\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m6478\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[39m, in \u001b[36m_datetimelike_compat.<locals>.new_func\u001b[39m\u001b[34m(values, axis, skipna, mask, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     mask = isna(values)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[32m    407\u001b[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\nanops.py:720\u001b[39m, in \u001b[36mnanmean\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m    718\u001b[39m count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n\u001b[32m    719\u001b[39m the_sum = values.sum(axis, dtype=dtype_sum)\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m the_sum = \u001b[43m_ensure_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthe_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    723\u001b[39m     count = cast(np.ndarray, count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Srijan\\Jupyter-Analysis\\mimic-iv-clinical-database-demo-2.2\\.venv\\Lib\\site-packages\\pandas\\core\\nanops.py:1701\u001b[39m, in \u001b[36m_ensure_numeric\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_float(x) \u001b[38;5;129;01mor\u001b[39;00m is_integer(x) \u001b[38;5;129;01mor\u001b[39;00m is_complex(x)):\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1700\u001b[39m         \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1701\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not convert string \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m to numeric\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1703\u001b[39m         x = \u001b[38;5;28mfloat\u001b[39m(x)\n",
      "\u001b[31mTypeError\u001b[39m: Could not convert string 'MM' to numeric"
     ]
    }
   ],
   "source": [
    "# 4. Run and Validate Output\n",
    "# Example synthetic data for demonstration\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'subject_id': [1, 1, 1, 2, 2, 2],\n",
    "    'time': [0, 1, 2, 0, 1, 2],\n",
    "    'age': [65, 65, 65, 70, 70, 70],\n",
    "    'gender': ['M', 'M', 'M', 'F', 'F', 'F'],\n",
    "    'value': [80, 85, np.nan, 90, 95, 100]\n",
    "})\n",
    "\n",
    "# ETL\n",
    "cleaned = clean_data(df)\n",
    "filled = forward_fill(cleaned)\n",
    "imputed = gaussian_impute(filled)\n",
    "\n",
    "# Feature Engineering\n",
    "windows = create_sliding_windows(imputed, window_size=2, time_col='time')\n",
    "agg = aggregate_features(windows, value_col='value_mean')\n",
    "\n",
    "# Modeling (Logistic Regression)\n",
    "X = agg[['value_mean', 'value_max']].fillna(0)\n",
    "y = np.array([0, 1, 0, 1])[:len(X)]  # synthetic outcome\n",
    "model, auroc, auprc = train_logistic_regression(X, y)\n",
    "print('AUROC:', auroc)\n",
    "print('AUPRC:', auprc)\n",
    "\n",
    "# Visualizations\n",
    "plot_missingness_heatmap(df)\n",
    "\n",
    "# Note: Other visualizations and models require more complex/specific data structures and are demonstrated in the full pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
